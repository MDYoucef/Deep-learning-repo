{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = da.from_npy_stack('/home/skyolia/JupyterProjects/segmentation/DRIVE/data/train_x')\n",
    "train_y = da.from_npy_stack('/home/skyolia/JupyterProjects/segmentation/DRIVE/data/train_y')\n",
    "test_x = da.from_npy_stack('/home/skyolia/JupyterProjects/segmentation/DRIVE/data/test_x')\n",
    "test_y = da.from_npy_stack('/home/skyolia/JupyterProjects/segmentation/DRIVE/data/test_y')\n",
    "train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block(input_layer, filters, norm=True, k=(3, 3)):\n",
    "    layer = tf.keras.layers.Conv2D(filters, kernel_size=k, padding='same', use_bias=not norm, kernel_initializer='glorot_normal')(input_layer)\n",
    "    if norm:\n",
    "        layer = tf.keras.layers.BatchNormalization()(layer)\n",
    "    layer = tf.keras.layers.Activation('elu')(layer)\n",
    "    return layer\n",
    "\n",
    "def build_unet(input_shape, n_filters=16, dropout=0.):\n",
    "    image_input = tf.keras.Input(shape=input_shape, name='input_layer')\n",
    "    \n",
    "    conv_1 = build_block(image_input, n_filters)\n",
    "    conv_2 = build_block(conv_1, n_filters)\n",
    "    conv_2 = build_block(conv_2, n_filters)\n",
    "    pool_1 = tf.keras.layers.AveragePooling2D(padding='same')(conv_2)\n",
    "    drop_1 = tf.keras.layers.SpatialDropout2D(dropout)(pool_1)\n",
    "    \n",
    "    conv_3 = build_block(drop_1, n_filters * 2)\n",
    "    conv_4 = build_block(conv_3, n_filters * 2)\n",
    "    conv_4 = build_block(conv_4, n_filters * 2)\n",
    "    pool_2 = tf.keras.layers.AveragePooling2D(padding='same')(conv_4)\n",
    "    drop_2 = tf.keras.layers.SpatialDropout2D(dropout)(pool_2)\n",
    "    \n",
    "    conv_5 = build_block(drop_2, n_filters * 4)\n",
    "    conv_6 = build_block(conv_5, n_filters * 4)\n",
    "    conv_6 = build_block(conv_6, n_filters * 4)\n",
    "    pool_3 = tf.keras.layers.AveragePooling2D(padding='same')(conv_6)\n",
    "    drop_3 = tf.keras.layers.SpatialDropout2D(dropout)(pool_3)\n",
    "    \n",
    "    conv_7 = build_block(drop_3, n_filters * 8)\n",
    "    conv_8 = build_block(conv_7, n_filters * 8)\n",
    "    conv_8 = build_block(conv_8, n_filters * 8)\n",
    "    pool_4 = tf.keras.layers.AveragePooling2D(padding='same')(conv_8)\n",
    "    drop_4 = tf.keras.layers.SpatialDropout2D(dropout)(pool_4)\n",
    "    \n",
    "    conv_9 = build_block(drop_4, n_filters * 16)\n",
    "    conv_10 = build_block(conv_9, n_filters * 16)\n",
    "    conv_10 = build_block(conv_10, n_filters * 16)\n",
    "    \n",
    "    upsp_1 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv_10) #(-1, 16, 16, 256)\n",
    "    upsp_1 = tf.keras.layers.concatenate([upsp_1, conv_8])#(-1, 16, 16, 384)\n",
    "    upsp_1 = tf.keras.layers.SpatialDropout2D(dropout)(upsp_1)#(-1, 16, 16, 384)\n",
    "    conv_11 = build_block(upsp_1, n_filters * 8)#(-1, 16, 16, 128)\n",
    "    conv_12 = build_block(conv_11, n_filters * 8)#(-1, 16, 16, 128)\n",
    "    conv_12 = build_block(conv_12, n_filters * 8)#(-1, 16, 16, 128)\n",
    "    \n",
    "    upsp_2 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv_12) #(-1, 32, 32, 128)\n",
    "    upsp_2 = tf.keras.layers.concatenate([upsp_2, conv_6]) #(-1, 32, 32, 192)\n",
    "    upsp_2 = tf.keras.layers.SpatialDropout2D(dropout)(upsp_2) #(-1, 32, 32, 192)\n",
    "    conv_13 = build_block(upsp_2, n_filters * 4) #(-1, 32, 32, 64)\n",
    "    conv_14 = build_block(conv_13, n_filters * 4) #(-1, 32, 32, 64)\n",
    "    conv_14 = build_block(conv_14, n_filters * 4) #(-1, 32, 32, 64)\n",
    "    \n",
    "    upsp_3 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv_14) #(-1, 64, 64, 64)\n",
    "    upsp_3 = tf.keras.layers.concatenate([upsp_3, conv_4]) #(-1, 64, 64, 96)\n",
    "    upsp_3 = tf.keras.layers.SpatialDropout2D(dropout)(upsp_3) #(-1, 64, 64, 96)\n",
    "    conv_15 = build_block(upsp_3, n_filters * 2) #(-1, 64, 64, 32)\n",
    "    conv_16 = build_block(conv_15, n_filters * 2) #(-1, 64, 64, 32)\n",
    "    conv_16 = build_block(conv_16, n_filters * 2) #(-1, 64, 64, 32)\n",
    "    \n",
    "    upsp_4 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv_16) #(-1, 128, 128, 32)\n",
    "    upsp_4 = tf.keras.layers.concatenate([upsp_4, conv_2])#(-1, 128, 128, 48)\n",
    "    upsp_4 = tf.keras.layers.SpatialDropout2D(dropout)(upsp_4)#(-1, 128, 128, 48)\n",
    "    conv_17 = build_block(upsp_4, n_filters)#(-1, 128, 128, 16)\n",
    "    conv_18 = build_block(conv_17, n_filters)#(-1, 128, 128, 16)\n",
    "    conv_18 = build_block(conv_18, n_filters)#(-1, 128, 128, 16)\n",
    "    \n",
    "    output = tf.keras.layers.Conv2D(1, (1, 1), kernel_initializer='glorot_normal', activation='sigmoid')(conv_18)\n",
    "    model = tf.keras.Model(inputs=image_input, outputs=output)\n",
    "    return model\n",
    "\n",
    "def random_crop(img, msk, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    #assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :], msk[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_x_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, batch_x.shape[3]))\n",
    "        batch_y_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, batch_x.shape[3]))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_x_crops[i], batch_y_crops[i] = random_crop(batch_x[i], batch_y[i], (crop_length, crop_length))\n",
    "        yield (batch_x_crops, batch_y_crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet(input_shape=(128, 128, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch_size, lr, filepath = 10000, 4, 0.001, \"day_1.weights.best.hdf5\"\n",
    "steps_per_epoch = int(np.ceil(train_y.shape[0]/batch_size))\n",
    "\n",
    "data_gen_args = dict(horizontal_flip=True, vertical_flip=True) #width_shift_range=0.1, height_shift_range=0.1, \n",
    "image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "image_datagen.fit(train_x, augment=True, seed=seed)\n",
    "mask_datagen.fit(train_y, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow(x=train_x, batch_size=batch_size, seed=seed)\n",
    "mask_generator = mask_datagen.flow(x=train_y, batch_size=batch_size, seed=seed)\n",
    "train_generator = crop_generator(zip(image_generator, mask_generator), 128)\n",
    "test_generator = crop_generator(tf.keras.preprocessing.image.ImageDataGenerator().flow(x=test_x, y=test_y, batch_size=batch_size), 128)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=os.getcwd())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam() # \n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=int(np.ceil(test_y.shape[0]/batch_size)),\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=12,\n",
    "                    shuffle=True,\n",
    "                    #initial_epoch=65,\n",
    "                    callbacks=[checkpoint, tb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
