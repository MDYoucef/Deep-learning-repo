{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.array as da\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "np.set_printoptions(threshold=10000, linewidth=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(sorted(glob.glob(\"/home/skyolia/JupyterProjects/data/BASE-CYTO/images etiquet√©es/*.bmp\")))\n",
    "x = np.asarray(sorted(glob.glob(\"/home/skyolia/JupyterProjects/data/BASE-CYTO/images originales/*.bmp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [ 0,  1,  2,  3,  4,  5,  6,  7,  8,9,\n",
    " 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    " 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
    " 30, 31, 32, 33, 34, 35, 53, 54, 55, 56,\n",
    " 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
    " 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,\n",
    " 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]\n",
    "test = [36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
    " 46, 47, 48, 49, 50, 51, 52]\n",
    "#train_x, train_y = x[train], y[train]\n",
    "#test_x, test_y = x[test], y[test]\n",
    "#train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, array([ 0,  1,  4,  5,  6,  7,\n",
       "         8,  9, 11, 12, 16, 17,\n",
       "        20, 22, 23, 24, 26, 27,\n",
       "        29, 31, 32, 34, 35, 38,\n",
       "        40, 41, 42, 43, 44, 45,\n",
       "        46, 47, 48, 49, 52, 53,\n",
       "        54, 55, 56, 58, 61, 63,\n",
       "        64, 66, 67, 69, 70, 72,\n",
       "        73, 74, 75, 76, 77, 78,\n",
       "        79, 80, 81, 82, 83, 85,\n",
       "        86]), array([ 2,  3, 10, 13, 14, 15,\n",
       "        18, 19, 21, 25, 28, 30,\n",
       "        33, 36, 37, 39, 50, 51,\n",
       "        57, 59, 60, 62, 65, 68,\n",
       "        71, 84]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd = np.random.choice(87, 61, False)\n",
    "#train_x, train_y = x[train_idx], y[train_idx]\n",
    "\n",
    "splitter = np.isin(np.arange(87), rnd, True)\n",
    "train, test = np.where(splitter)[0], np.where(~splitter)[0]\n",
    "\n",
    "np.sum(splitter), train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<rechunk-merge, shape=(88970, 32, 32, 3), dtype=int64, chunksize=(15000, 32, 32, 3)>,\n",
       " dask.array<rechunk-merge, shape=(88970, 32, 32, 1), dtype=int64, chunksize=(15000, 32, 32, 1)>,\n",
       " dask.array<stack, shape=(17, 768, 1024, 3), dtype=int64, chunksize=(1, 768, 1024, 3)>,\n",
       " dask.array<stack, shape=(17, 768, 1024, 1), dtype=int64, chunksize=(1, 768, 1024, 1)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def padding(obj, i):\n",
    "    pad_size = np.asarray(obj) - np.asarray(i.shape)\n",
    "    padded = np.pad(i, ((0, pad_size[0]), (0, pad_size[1]), (0, pad_size[2])), mode='edge')\n",
    "    return padded\n",
    "\n",
    "def extract_patch_single_image(i, crop_size):\n",
    "    crops = []\n",
    "    height, width = i.shape[0], i.shape[1]\n",
    "    for h in range(0, height, 25):\n",
    "        for w in range(0, width, 25):\n",
    "            image = i[h:h+crop_size[0], w:w+crop_size[1], :]\n",
    "            image = padding((crop_size[0], crop_size[1], image.shape[2]), image) if image.shape[0:2] != crop_size else image\n",
    "            crops.append(image)\n",
    "    return np.asarray(crops)\n",
    "\n",
    "def load_save(path, mode, crop_size, new_shape=None):\n",
    "    #image = Image.open(path).convert(mode)\n",
    "    image = scipy.ndimage.imread(path, mode=mode)\n",
    "    image = np.asarray(image)#/255.0\n",
    "    image = image[:, :, np.newaxis] if image.ndim == 2 else image\n",
    "    image = extract_patch_single_image(image, crop_size) if crop_size else image\n",
    "    image = resize(image, new_shape, preserve_range = True) if new_shape else image\n",
    "    return image#.astype(int)\n",
    "\n",
    "def data_processing(data_paths, crop_size, new_shape, mode, chunk, stack=True):\n",
    "    x = []\n",
    "    for i in data_paths:\n",
    "        image = dask.delayed(load_save)(i, mode, crop_size, new_shape=None)\n",
    "        x.append(da.from_delayed(image, new_shape, dtype=int))\n",
    "    res = da.stack(x, axis=0).rechunk(chunk) if stack else da.concatenate(x, axis=0).rechunk(chunk)\n",
    "    return res\n",
    "\n",
    "train_x = data_processing(x[train], (32, 32), (1271, 32, 32, 3), 'RGB', (15000, 32, 32, 3), False)\n",
    "train_y = data_processing(y[train], (32, 32), (1271, 32, 32, 1), 'L', (15000, 32, 32, 1), False)\n",
    "#colored_train_y = data_processing(masks[train], (32, 32), (7400, 32, 32, 3), 'RGB', (10000, 32, 32, 3), False)\n",
    "test_x = data_processing(x[test], None, (768, 1024, 3), 'RGB', (1, 768, 1024, 3))\n",
    "test_y = data_processing(y[test], None, (768, 1024, 1), 'L', (1, 768, 1024, 1))\n",
    "#colored_test_y = data_processing(masks[test], None, (768, 1024, 3), 'RGB', (1, 768, 1024, 3))\n",
    "train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.32 s, sys: 2.12 s, total: 5.44 s\n",
      "Wall time: 9.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  0,  59, 117, 177],\n",
       "       dtype=uint8), array([  0,  59, 117, 177],\n",
       "       dtype=uint8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_y = da.where(train_y==118, 117, train_y)\n",
    "%time da.unique(train_y).compute(), da.unique(test_y).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dask.array<truediv, shape=(88970, 32, 32, 3), dtype=float64, chunksize=(15000, 32, 32, 3)>,\n",
       " dask.array<concatenate, shape=(88970, 32, 32, 4), dtype=float64, chunksize=(15000, 32, 32, 1)>,\n",
       " dask.array<truediv, shape=(17, 768, 1024, 3), dtype=float64, chunksize=(1, 768, 1024, 3)>,\n",
       " dask.array<concatenate, shape=(17, 768, 1024, 4), dtype=float64, chunksize=(1, 768, 1024, 1)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_labels_convertion(y):\n",
    "    res = []\n",
    "    for i in da.unique(y).compute():\n",
    "        res.append(da.where(y==i, 1., 0))\n",
    "    return da.concatenate(res, axis=3)\n",
    "\n",
    "train_y = one_hot_labels_convertion(train_y)\n",
    "test_y = one_hot_labels_convertion(test_y)\n",
    "train_x, test_x = train_x/255., test_x/255.\n",
    "train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 1 432         input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 1 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, None, 1 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 2304        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 1 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 1 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, None, None, 1 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d (SpatialDropo (None, None, None, 1 0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 4608        spatial_dropout2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 3 9216        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 3 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 3 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 3 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_1 (SpatialDro (None, None, None, 3 0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 6 18432       spatial_dropout2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 6 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 6 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 6 36864       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 6 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, None, None, 6 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_2 (SpatialDro (None, None, None, 6 0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 73728       spatial_dropout2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 1 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 147456      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 1 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, None, None, 1 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 1 0           up_sampling2d[0][0]              \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 110592      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 6 36864       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_3 (SpatialDro (None, None, None, 6 0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 6 0           spatial_dropout2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 9 0           up_sampling2d_1[0][0]            \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 3 27648       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 3 128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 3 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 3 9216        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 3 128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 3 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_4 (SpatialDro (None, None, None, 3 0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, None, None, 3 0           spatial_dropout2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           up_sampling2d_2[0][0]            \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 1 6912        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 1 64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 1 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 2304        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 1 64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 1 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_5 (SpatialDro (None, None, None, 1 0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 68          spatial_dropout2d_5[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 489,460\n",
      "Trainable params: 488,052\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_block(input_layer, filters, norm=True, k=(3, 3)):\n",
    "    layer = tf.keras.layers.Conv2D(filters, kernel_size=k, padding='same', use_bias=not norm, kernel_initializer='glorot_normal')(input_layer)\n",
    "    if norm:\n",
    "        layer = tf.keras.layers.BatchNormalization()(layer)\n",
    "    layer = tf.keras.layers.Activation('elu')(layer)\n",
    "    return layer\n",
    "\n",
    "def build_unet(input_shape, n_filters, dropout=0.1):\n",
    "    image_input = tf.keras.Input(shape=input_shape, name='input_layer')\n",
    "    \n",
    "    conv_1 = build_block(image_input, n_filters) #(32, 32, 16)\n",
    "    conv_2 = build_block(conv_1, n_filters) #(32, 32, 16)\n",
    "    pool_1 = tf.keras.layers.AveragePooling2D(padding='same')(conv_2)#(16, 16, 16)\n",
    "    drop_1 = tf.keras.layers.SpatialDropout2D(dropout)(pool_1)\n",
    "    \n",
    "    conv_3 = build_block(drop_1, n_filters * 2)#(16, 16, 32)\n",
    "    conv_4 = build_block(conv_3, n_filters * 2)#(16, 16, 32)\n",
    "    pool_2 = tf.keras.layers.AveragePooling2D(padding='same')(conv_4)#(8, 8, 32)\n",
    "    drop_2 = tf.keras.layers.SpatialDropout2D(dropout)(pool_2)\n",
    "    \n",
    "    conv_5 = build_block(drop_2, n_filters * 4)#(8, 8, 64)\n",
    "    conv_6 = build_block(conv_5, n_filters * 4) #(8, 8, 64)\n",
    "    pool_3 = tf.keras.layers.AveragePooling2D(padding='same')(conv_6)#(4, 4, 64)\n",
    "    drop_3 = tf.keras.layers.SpatialDropout2D(dropout)(pool_3)\n",
    "    \n",
    "    conv_7 = build_block(drop_3, n_filters * 8)#(4, 4, 128)\n",
    "    conv_8 = build_block(conv_7, n_filters * 8) #(4, 4, 128)\n",
    "    \n",
    "    upsp_1 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv_8) #(-1, 8, 8, 128)\n",
    "    upsp_1 = tf.keras.layers.concatenate([upsp_1, conv_6]) #(-1, 8, 8, 192)\n",
    "    conv_9 = build_block(upsp_1, n_filters * 4) #(-1, 8, 8, 64)\n",
    "    conv_10 = build_block(conv_9, n_filters * 4)\n",
    "    drop_7 = tf.keras.layers.SpatialDropout2D(dropout)(conv_10)\n",
    "    \n",
    "    upsp_3 = tf.keras.layers.UpSampling2D(size=(2, 2))(drop_7) #(-1, 16, 16, 128)\n",
    "    upsp_3 = tf.keras.layers.concatenate([upsp_3, conv_4]) #(-1, 16, 16, 192)\n",
    "    conv_15 = build_block(upsp_3, n_filters * 2) #(-1, 16, 16, 64)\n",
    "    conv_16 = build_block(conv_15, n_filters * 2)\n",
    "    drop_7 = tf.keras.layers.SpatialDropout2D(dropout)(conv_16)\n",
    "    \n",
    "    upsp_4 = tf.keras.layers.UpSampling2D(size=(2, 2))(drop_7) #(-1, 32, 32, 64)\n",
    "    upsp_4 = tf.keras.layers.concatenate([upsp_4, conv_2])#(-1, 32, 32, 92)\n",
    "    conv_17 = build_block(upsp_4, n_filters)#(-1, 32, 32, 32)\n",
    "    conv_18 = build_block(conv_17, n_filters)\n",
    "    drop_8 = tf.keras.layers.SpatialDropout2D(dropout)(conv_18)\n",
    "    \n",
    "    output = tf.keras.layers.Conv2D(4, (1, 1), kernel_initializer='glorot_normal', activation='softmax')(drop_8)\n",
    "    model = tf.keras.Model(inputs=image_input, outputs=output)\n",
    "    return model\n",
    "        \n",
    "model = build_unet(input_shape=(None, None, 3), n_filters=16)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 7s 422ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2183622774832389, 0.9225624729605282]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"day_1.weights.best.hdf5\")\n",
    "opt = tf.keras.optimizers.Adam() # \n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "scores = model.evaluate(test_x, test_y, batch_size=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "Epoch 1/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 74.0348 - acc: 0.9265\n",
      "Epoch 00001: val_acc improved from -inf to 0.83733, saving model to day_1.weights.best.hdf5\n",
      "348/348 [==============================] - 132s 378ms/step - loss: 73.8912 - acc: 0.9266 - val_loss: 0.4674 - val_acc: 0.8373\n",
      "Epoch 2/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 46.7933 - acc: 0.9419\n",
      "Epoch 00002: val_acc improved from 0.83733 to 0.86841, saving model to day_1.weights.best.hdf5\n",
      "348/348 [==============================] - 55s 157ms/step - loss: 46.7065 - acc: 0.9420 - val_loss: 0.4057 - val_acc: 0.8684\n",
      "Epoch 3/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 40.1355 - acc: 0.9480\n",
      "Epoch 00003: val_acc improved from 0.86841 to 0.91337, saving model to day_1.weights.best.hdf5\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 40.0763 - acc: 0.9480 - val_loss: 0.2553 - val_acc: 0.9134\n",
      "Epoch 4/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 36.6865 - acc: 0.9512\n",
      "Epoch 00004: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 36.6210 - acc: 0.9513 - val_loss: 0.3381 - val_acc: 0.8884\n",
      "Epoch 5/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 34.8039 - acc: 0.9532\n",
      "Epoch 00005: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 34.7479 - acc: 0.9532 - val_loss: 0.5027 - val_acc: 0.8035\n",
      "Epoch 6/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 32.7484 - acc: 0.9554\n",
      "Epoch 00006: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 32.7030 - acc: 0.9554 - val_loss: 0.6172 - val_acc: 0.7227\n",
      "Epoch 7/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 31.9411 - acc: 0.9566\n",
      "Epoch 00007: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 31.8857 - acc: 0.9566 - val_loss: 0.4005 - val_acc: 0.8391\n",
      "Epoch 8/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 31.2345 - acc: 0.9573\n",
      "Epoch 00008: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 31.1930 - acc: 0.9573 - val_loss: 1.0389 - val_acc: 0.6568\n",
      "Epoch 9/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 30.1790 - acc: 0.9587\n",
      "Epoch 00009: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 30.1247 - acc: 0.9588 - val_loss: 0.3354 - val_acc: 0.8670\n",
      "Epoch 10/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 29.3333 - acc: 0.9596\n",
      "Epoch 00010: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 29.3056 - acc: 0.9595 - val_loss: 0.4704 - val_acc: 0.8365\n",
      "Epoch 11/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 28.9984 - acc: 0.9600\n",
      "Epoch 00011: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 28.9491 - acc: 0.9600 - val_loss: 0.9048 - val_acc: 0.6829\n",
      "Epoch 12/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 28.4268 - acc: 0.9610\n",
      "Epoch 00012: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 28.3912 - acc: 0.9609 - val_loss: 0.3047 - val_acc: 0.8802\n",
      "Epoch 13/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 28.0774 - acc: 0.9613\n",
      "Epoch 00013: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 28.0419 - acc: 0.9613 - val_loss: 0.4682 - val_acc: 0.8607\n",
      "Epoch 14/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 27.3871 - acc: 0.9622\n",
      "Epoch 00014: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 27.3489 - acc: 0.9622 - val_loss: 0.5762 - val_acc: 0.7715\n",
      "Epoch 15/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 26.9777 - acc: 0.9628\n",
      "Epoch 00015: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 26.9354 - acc: 0.9628 - val_loss: 0.7641 - val_acc: 0.6366\n",
      "Epoch 16/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 26.9322 - acc: 0.9627\n",
      "Epoch 00016: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 26.8859 - acc: 0.9627 - val_loss: 0.4284 - val_acc: 0.8266\n",
      "Epoch 17/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 26.4927 - acc: 0.9633\n",
      "Epoch 00017: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 162ms/step - loss: 26.4397 - acc: 0.9633 - val_loss: 0.4313 - val_acc: 0.8387\n",
      "Epoch 18/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 26.4458 - acc: 0.9634\n",
      "Epoch 00018: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 26.4027 - acc: 0.9634 - val_loss: 0.3767 - val_acc: 0.8458\n",
      "Epoch 19/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 25.9803 - acc: 0.9639\n",
      "Epoch 00019: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 162ms/step - loss: 25.9366 - acc: 0.9639 - val_loss: 0.3759 - val_acc: 0.8413\n",
      "Epoch 20/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 25.2190 - acc: 0.9651\n",
      "Epoch 00020: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 162ms/step - loss: 25.2094 - acc: 0.9650 - val_loss: 0.6727 - val_acc: 0.7767\n",
      "Epoch 21/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 25.0919 - acc: 0.9651\n",
      "Epoch 00021: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 25.0586 - acc: 0.9651 - val_loss: 0.3239 - val_acc: 0.8867\n",
      "Epoch 22/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 24.8077 - acc: 0.9656\n",
      "Epoch 00022: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 24.7666 - acc: 0.9656 - val_loss: 1.2958 - val_acc: 0.6038\n",
      "Epoch 23/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 24.9705 - acc: 0.9652\n",
      "Epoch 00023: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 162ms/step - loss: 24.9306 - acc: 0.9652 - val_loss: 0.8372 - val_acc: 0.6913\n",
      "Epoch 24/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 24.4522 - acc: 0.9660\n",
      "Epoch 00024: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 24.4216 - acc: 0.9660 - val_loss: 0.7845 - val_acc: 0.7224\n",
      "Epoch 25/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 24.5901 - acc: 0.9657\n",
      "Epoch 00025: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 24.5821 - acc: 0.9657 - val_loss: 0.4520 - val_acc: 0.7964\n",
      "Epoch 26/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 24.1832 - acc: 0.9663\n",
      "Epoch 00026: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 24.1411 - acc: 0.9664 - val_loss: 1.0492 - val_acc: 0.6823\n",
      "Epoch 27/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.8322 - acc: 0.9668\n",
      "Epoch 00027: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 23.8070 - acc: 0.9668 - val_loss: 0.6268 - val_acc: 0.7674\n",
      "Epoch 28/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.5489 - acc: 0.9671\n",
      "Epoch 00028: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 162ms/step - loss: 23.5231 - acc: 0.9671 - val_loss: 0.8332 - val_acc: 0.7375\n",
      "Epoch 29/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.6228 - acc: 0.9670\n",
      "Epoch 00029: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 23.5987 - acc: 0.9670 - val_loss: 0.4113 - val_acc: 0.8354\n",
      "Epoch 30/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.3071 - acc: 0.9676\n",
      "Epoch 00030: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 162ms/step - loss: 23.2916 - acc: 0.9675 - val_loss: 1.3250 - val_acc: 0.6314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.1459 - acc: 0.9675\n",
      "Epoch 00031: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 23.1185 - acc: 0.9675 - val_loss: 0.2862 - val_acc: 0.8922\n",
      "Epoch 32/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.1750 - acc: 0.9677\n",
      "Epoch 00032: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 23.1348 - acc: 0.9677 - val_loss: 0.4121 - val_acc: 0.8392\n",
      "Epoch 33/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 23.0939 - acc: 0.9677\n",
      "Epoch 00033: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 23.0690 - acc: 0.9677 - val_loss: 0.9567 - val_acc: 0.6743\n",
      "Epoch 34/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 22.9594 - acc: 0.9677\n",
      "Epoch 00034: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 22.9346 - acc: 0.9677 - val_loss: 1.4663 - val_acc: 0.5721\n",
      "Epoch 35/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 22.5425 - acc: 0.9684\n",
      "Epoch 00035: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 22.5105 - acc: 0.9684 - val_loss: 1.3821 - val_acc: 0.6186\n",
      "Epoch 36/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 22.4954 - acc: 0.9685\n",
      "Epoch 00036: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 22.4793 - acc: 0.9684 - val_loss: 1.1505 - val_acc: 0.6333\n",
      "Epoch 37/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 22.2706 - acc: 0.9688\n",
      "Epoch 00037: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 22.2598 - acc: 0.9688 - val_loss: 1.0577 - val_acc: 0.6981\n",
      "Epoch 38/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 22.1847 - acc: 0.9690\n",
      "Epoch 00038: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 22.1419 - acc: 0.9691 - val_loss: 0.5115 - val_acc: 0.7811\n",
      "Epoch 39/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.9394 - acc: 0.9692\n",
      "Epoch 00039: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.9173 - acc: 0.9692 - val_loss: 1.6944 - val_acc: 0.6578\n",
      "Epoch 40/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 22.2527 - acc: 0.9690\n",
      "Epoch 00040: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 22.2272 - acc: 0.9690 - val_loss: 0.5347 - val_acc: 0.8074\n",
      "Epoch 41/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.6087 - acc: 0.9696\n",
      "Epoch 00041: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.5869 - acc: 0.9696 - val_loss: 1.7502 - val_acc: 0.5707\n",
      "Epoch 42/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.5321 - acc: 0.9698\n",
      "Epoch 00042: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.5123 - acc: 0.9698 - val_loss: 1.8077 - val_acc: 0.6323\n",
      "Epoch 43/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.4890 - acc: 0.9699\n",
      "Epoch 00043: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.4740 - acc: 0.9698 - val_loss: 0.7719 - val_acc: 0.8033\n",
      "Epoch 44/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.5069 - acc: 0.9696\n",
      "Epoch 00044: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.4696 - acc: 0.9696 - val_loss: 0.8134 - val_acc: 0.7316\n",
      "Epoch 45/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.3036 - acc: 0.9700\n",
      "Epoch 00045: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.2766 - acc: 0.9701 - val_loss: 1.2071 - val_acc: 0.7001\n",
      "Epoch 46/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.2046 - acc: 0.9701\n",
      "Epoch 00046: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.1788 - acc: 0.9701 - val_loss: 0.5564 - val_acc: 0.7662\n",
      "Epoch 47/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.9551 - acc: 0.9705\n",
      "Epoch 00047: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.9309 - acc: 0.9705 - val_loss: 0.2970 - val_acc: 0.8915\n",
      "Epoch 48/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.1030 - acc: 0.9702\n",
      "Epoch 00048: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.0762 - acc: 0.9702 - val_loss: 0.3123 - val_acc: 0.8903\n",
      "Epoch 49/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 21.0332 - acc: 0.9704\n",
      "Epoch 00049: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 21.0006 - acc: 0.9704 - val_loss: 1.0061 - val_acc: 0.6976\n",
      "Epoch 50/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.7052 - acc: 0.9708\n",
      "Epoch 00050: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.6738 - acc: 0.9708 - val_loss: 0.5564 - val_acc: 0.7877\n",
      "Epoch 51/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.7813 - acc: 0.9706\n",
      "Epoch 00051: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.7651 - acc: 0.9706 - val_loss: 0.6343 - val_acc: 0.7869\n",
      "Epoch 52/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.5995 - acc: 0.9709\n",
      "Epoch 00052: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.5725 - acc: 0.9709 - val_loss: 1.5359 - val_acc: 0.6364\n",
      "Epoch 53/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.4416 - acc: 0.9710\n",
      "Epoch 00053: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.4068 - acc: 0.9710 - val_loss: 0.3583 - val_acc: 0.8573\n",
      "Epoch 54/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.4445 - acc: 0.9710\n",
      "Epoch 00054: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.4243 - acc: 0.9710 - val_loss: 1.8688 - val_acc: 0.6006\n",
      "Epoch 55/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.2528 - acc: 0.9714\n",
      "Epoch 00055: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.2218 - acc: 0.9714 - val_loss: 0.5246 - val_acc: 0.8182\n",
      "Epoch 56/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.1900 - acc: 0.9714\n",
      "Epoch 00056: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.1632 - acc: 0.9714 - val_loss: 0.7912 - val_acc: 0.7252\n",
      "Epoch 57/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.0519 - acc: 0.9715\n",
      "Epoch 00057: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.0205 - acc: 0.9715 - val_loss: 0.3573 - val_acc: 0.8783\n",
      "Epoch 58/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 20.1061 - acc: 0.9715\n",
      "Epoch 00058: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 20.0943 - acc: 0.9714 - val_loss: 1.0080 - val_acc: 0.7217\n",
      "Epoch 59/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.9895 - acc: 0.9716\n",
      "Epoch 00059: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.9608 - acc: 0.9716 - val_loss: 1.6210 - val_acc: 0.6593\n",
      "Epoch 60/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.7778 - acc: 0.9718\n",
      "Epoch 00060: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.7495 - acc: 0.9718 - val_loss: 0.3934 - val_acc: 0.8723\n",
      "Epoch 61/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/348 [============================>.] - ETA: 0s - loss: 19.8759 - acc: 0.9717\n",
      "Epoch 00061: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 19.8542 - acc: 0.9717 - val_loss: 3.4332 - val_acc: 0.5366\n",
      "Epoch 62/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.8331 - acc: 0.9719\n",
      "Epoch 00062: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 19.8053 - acc: 0.9719 - val_loss: 1.8050 - val_acc: 0.6448\n",
      "Epoch 63/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.7088 - acc: 0.9720\n",
      "Epoch 00063: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 19.7122 - acc: 0.9720 - val_loss: 0.4186 - val_acc: 0.8406\n",
      "Epoch 64/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.6710 - acc: 0.9720\n",
      "Epoch 00064: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 19.6347 - acc: 0.9720 - val_loss: 1.3518 - val_acc: 0.6748\n",
      "Epoch 65/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.6314 - acc: 0.9722\n",
      "Epoch 00065: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 19.6001 - acc: 0.9722 - val_loss: 0.3107 - val_acc: 0.8936\n",
      "Epoch 66/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.4340 - acc: 0.9722\n",
      "Epoch 00066: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.4100 - acc: 0.9722 - val_loss: 0.4201 - val_acc: 0.8289\n",
      "Epoch 67/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.2979 - acc: 0.9726\n",
      "Epoch 00067: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.2651 - acc: 0.9726 - val_loss: 0.6711 - val_acc: 0.7714\n",
      "Epoch 68/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.3033 - acc: 0.9725\n",
      "Epoch 00068: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.2826 - acc: 0.9725 - val_loss: 0.6330 - val_acc: 0.7534\n",
      "Epoch 69/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.3535 - acc: 0.9725\n",
      "Epoch 00069: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.3244 - acc: 0.9725 - val_loss: 0.5655 - val_acc: 0.7834\n",
      "Epoch 70/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.3759 - acc: 0.9724\n",
      "Epoch 00070: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 19.3468 - acc: 0.9724 - val_loss: 0.3684 - val_acc: 0.8585\n",
      "Epoch 71/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.9610 - acc: 0.9728\n",
      "Epoch 00071: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.9347 - acc: 0.9728 - val_loss: 1.1526 - val_acc: 0.7131\n",
      "Epoch 72/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 19.0192 - acc: 0.9729\n",
      "Epoch 00072: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.9958 - acc: 0.9729 - val_loss: 0.2792 - val_acc: 0.9009\n",
      "Epoch 73/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.9767 - acc: 0.9728\n",
      "Epoch 00073: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.9512 - acc: 0.9728 - val_loss: 0.4430 - val_acc: 0.8323\n",
      "Epoch 74/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.8792 - acc: 0.9731\n",
      "Epoch 00074: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.8649 - acc: 0.9731 - val_loss: 0.6504 - val_acc: 0.7878\n",
      "Epoch 75/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.6914 - acc: 0.9732\n",
      "Epoch 00075: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.6641 - acc: 0.9732 - val_loss: 0.3077 - val_acc: 0.8957\n",
      "Epoch 76/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.7549 - acc: 0.9731\n",
      "Epoch 00076: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.7213 - acc: 0.9731 - val_loss: 0.6376 - val_acc: 0.8114\n",
      "Epoch 77/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.7113 - acc: 0.9733\n",
      "Epoch 00077: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.6786 - acc: 0.9733 - val_loss: 0.6649 - val_acc: 0.7861\n",
      "Epoch 78/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.8646 - acc: 0.9730\n",
      "Epoch 00078: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.8317 - acc: 0.9730 - val_loss: 0.3108 - val_acc: 0.8889\n",
      "Epoch 79/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.6316 - acc: 0.9734\n",
      "Epoch 00079: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.6233 - acc: 0.9734 - val_loss: 0.6937 - val_acc: 0.7655\n",
      "Epoch 80/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.4454 - acc: 0.9734\n",
      "Epoch 00080: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.4214 - acc: 0.9734 - val_loss: 1.1878 - val_acc: 0.6906\n",
      "Epoch 81/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.2800 - acc: 0.9736\n",
      "Epoch 00081: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.2522 - acc: 0.9736 - val_loss: 0.7932 - val_acc: 0.7592\n",
      "Epoch 82/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.1488 - acc: 0.9738\n",
      "Epoch 00082: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.1077 - acc: 0.9739 - val_loss: 0.2914 - val_acc: 0.8953\n",
      "Epoch 83/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.3818 - acc: 0.9737\n",
      "Epoch 00083: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 18.3704 - acc: 0.9737 - val_loss: 1.2902 - val_acc: 0.7281\n",
      "Epoch 84/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.1551 - acc: 0.9737\n",
      "Epoch 00084: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.1477 - acc: 0.9737 - val_loss: 0.4687 - val_acc: 0.8249\n",
      "Epoch 85/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.3203 - acc: 0.9735\n",
      "Epoch 00085: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.2956 - acc: 0.9735 - val_loss: 1.0607 - val_acc: 0.7485\n",
      "Epoch 86/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.1492 - acc: 0.9738\n",
      "Epoch 00086: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.1435 - acc: 0.9738 - val_loss: 0.3017 - val_acc: 0.8992\n",
      "Epoch 87/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.0924 - acc: 0.9739\n",
      "Epoch 00087: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.0669 - acc: 0.9739 - val_loss: 2.6123 - val_acc: 0.6222\n",
      "Epoch 88/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.9635 - acc: 0.9741\n",
      "Epoch 00088: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.9418 - acc: 0.9741 - val_loss: 0.3615 - val_acc: 0.8974\n",
      "Epoch 89/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 18.0393 - acc: 0.9740\n",
      "Epoch 00089: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 18.0185 - acc: 0.9740 - val_loss: 0.2897 - val_acc: 0.9057\n",
      "Epoch 90/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.8164 - acc: 0.9742\n",
      "Epoch 00090: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.7981 - acc: 0.9742 - val_loss: 1.3642 - val_acc: 0.6531\n",
      "Epoch 91/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.9261 - acc: 0.9742\n",
      "Epoch 00091: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 17.9099 - acc: 0.9742 - val_loss: 0.2704 - val_acc: 0.9113\n",
      "Epoch 92/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.8421 - acc: 0.9742\n",
      "Epoch 00092: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.8264 - acc: 0.9741 - val_loss: 0.7934 - val_acc: 0.7485\n",
      "Epoch 93/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.7816 - acc: 0.9744\n",
      "Epoch 00093: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 17.7610 - acc: 0.9744 - val_loss: 0.3919 - val_acc: 0.8774\n",
      "Epoch 94/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.7404 - acc: 0.9745\n",
      "Epoch 00094: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.7152 - acc: 0.9745 - val_loss: 0.3386 - val_acc: 0.8746\n",
      "Epoch 95/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.9510 - acc: 0.9741\n",
      "Epoch 00095: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.9326 - acc: 0.9740 - val_loss: 0.3173 - val_acc: 0.8926\n",
      "Epoch 96/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.5323 - acc: 0.9746\n",
      "Epoch 00096: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.5058 - acc: 0.9746 - val_loss: 0.3139 - val_acc: 0.8839\n",
      "Epoch 97/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.5963 - acc: 0.9745\n",
      "Epoch 00097: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.5777 - acc: 0.9745 - val_loss: 0.5284 - val_acc: 0.8168\n",
      "Epoch 98/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.6067 - acc: 0.9746\n",
      "Epoch 00098: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.5926 - acc: 0.9746 - val_loss: 0.4143 - val_acc: 0.8895\n",
      "Epoch 99/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.5038 - acc: 0.9748\n",
      "Epoch 00099: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.4754 - acc: 0.9748 - val_loss: 0.3669 - val_acc: 0.8684\n",
      "Epoch 100/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.3561 - acc: 0.9748\n",
      "Epoch 00100: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.3251 - acc: 0.9749 - val_loss: 0.9042 - val_acc: 0.7320\n",
      "Epoch 101/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.4149 - acc: 0.9748\n",
      "Epoch 00101: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.3863 - acc: 0.9748 - val_loss: 0.4184 - val_acc: 0.8489\n",
      "Epoch 102/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.4312 - acc: 0.9748\n",
      "Epoch 00102: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.4050 - acc: 0.9748 - val_loss: 0.4043 - val_acc: 0.8699\n",
      "Epoch 103/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.0754 - acc: 0.9752\n",
      "Epoch 00103: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.0607 - acc: 0.9752 - val_loss: 0.8212 - val_acc: 0.7643\n",
      "Epoch 104/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.3791 - acc: 0.9747\n",
      "Epoch 00104: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.3483 - acc: 0.9748 - val_loss: 0.5436 - val_acc: 0.8112\n",
      "Epoch 105/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.3385 - acc: 0.9749\n",
      "Epoch 00105: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.3264 - acc: 0.9749 - val_loss: 0.3043 - val_acc: 0.9033\n",
      "Epoch 106/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.9678 - acc: 0.9753\n",
      "Epoch 00106: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.9424 - acc: 0.9753 - val_loss: 0.3986 - val_acc: 0.8652\n",
      "Epoch 107/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 17.1729 - acc: 0.9751\n",
      "Epoch 00107: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 17.1527 - acc: 0.9751 - val_loss: 0.3203 - val_acc: 0.8889\n",
      "Epoch 108/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.9943 - acc: 0.9753\n",
      "Epoch 00108: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.9752 - acc: 0.9753 - val_loss: 0.5159 - val_acc: 0.8259\n",
      "Epoch 109/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.8759 - acc: 0.9755\n",
      "Epoch 00109: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.8535 - acc: 0.9755 - val_loss: 0.5882 - val_acc: 0.8111\n",
      "Epoch 110/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.8807 - acc: 0.9755\n",
      "Epoch 00110: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.8527 - acc: 0.9755 - val_loss: 0.3806 - val_acc: 0.8669\n",
      "Epoch 111/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.9371 - acc: 0.9754\n",
      "Epoch 00111: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.9171 - acc: 0.9754 - val_loss: 0.3039 - val_acc: 0.9012\n",
      "Epoch 112/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.9317 - acc: 0.9753\n",
      "Epoch 00112: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.9065 - acc: 0.9753 - val_loss: 0.5309 - val_acc: 0.8246\n",
      "Epoch 113/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.7344 - acc: 0.9757\n",
      "Epoch 00113: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.7099 - acc: 0.9757 - val_loss: 0.6019 - val_acc: 0.8260\n",
      "Epoch 114/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.5629 - acc: 0.9758\n",
      "Epoch 00114: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.5403 - acc: 0.9758 - val_loss: 0.3553 - val_acc: 0.8835\n",
      "Epoch 115/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.8464 - acc: 0.9756\n",
      "Epoch 00115: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.8293 - acc: 0.9756 - val_loss: 0.5179 - val_acc: 0.7991\n",
      "Epoch 116/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.7460 - acc: 0.9756\n",
      "Epoch 00116: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.7378 - acc: 0.9755 - val_loss: 0.3714 - val_acc: 0.8715\n",
      "Epoch 117/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.6237 - acc: 0.9760\n",
      "Epoch 00117: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.6011 - acc: 0.9760 - val_loss: 0.4080 - val_acc: 0.8651\n",
      "Epoch 118/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.6012 - acc: 0.9759\n",
      "Epoch 00118: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.5795 - acc: 0.9759 - val_loss: 0.4842 - val_acc: 0.8337\n",
      "Epoch 119/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.5608 - acc: 0.9759\n",
      "Epoch 00119: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.5359 - acc: 0.9759 - val_loss: 0.4595 - val_acc: 0.8333\n",
      "Epoch 120/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.3585 - acc: 0.9761\n",
      "Epoch 00120: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.3394 - acc: 0.9761 - val_loss: 0.4070 - val_acc: 0.8501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.4559 - acc: 0.9761\n",
      "Epoch 00121: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.4398 - acc: 0.9761 - val_loss: 0.4774 - val_acc: 0.8386\n",
      "Epoch 122/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.3568 - acc: 0.9762\n",
      "Epoch 00122: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.3322 - acc: 0.9762 - val_loss: 0.4213 - val_acc: 0.8540\n",
      "Epoch 123/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.3461 - acc: 0.9762\n",
      "Epoch 00123: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.3262 - acc: 0.9762 - val_loss: 0.3990 - val_acc: 0.8751\n",
      "Epoch 124/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.5453 - acc: 0.9760\n",
      "Epoch 00124: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.5156 - acc: 0.9760 - val_loss: 0.2966 - val_acc: 0.9110\n",
      "Epoch 125/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.3062 - acc: 0.9763\n",
      "Epoch 00125: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.2805 - acc: 0.9763 - val_loss: 0.3915 - val_acc: 0.8699\n",
      "Epoch 126/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.2202 - acc: 0.9763\n",
      "Epoch 00126: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.1985 - acc: 0.9763 - val_loss: 0.3803 - val_acc: 0.8788\n",
      "Epoch 127/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.1211 - acc: 0.9762\n",
      "Epoch 00127: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.0927 - acc: 0.9763 - val_loss: 0.4076 - val_acc: 0.8604\n",
      "Epoch 128/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.1872 - acc: 0.9764\n",
      "Epoch 00128: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.1662 - acc: 0.9764 - val_loss: 0.4989 - val_acc: 0.8326\n",
      "Epoch 129/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.1745 - acc: 0.9765\n",
      "Epoch 00129: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.1577 - acc: 0.9765 - val_loss: 0.5555 - val_acc: 0.8250\n",
      "Epoch 130/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.2095 - acc: 0.9764\n",
      "Epoch 00130: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.2045 - acc: 0.9763 - val_loss: 0.7265 - val_acc: 0.7427\n",
      "Epoch 131/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 15.9235 - acc: 0.9766\n",
      "Epoch 00131: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 15.9084 - acc: 0.9766 - val_loss: 0.3962 - val_acc: 0.8767\n",
      "Epoch 132/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.1881 - acc: 0.9764\n",
      "Epoch 00132: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 16.1631 - acc: 0.9764 - val_loss: 0.5521 - val_acc: 0.8182\n",
      "Epoch 133/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 15.9424 - acc: 0.9767\n",
      "Epoch 00133: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 161ms/step - loss: 15.9173 - acc: 0.9767 - val_loss: 0.3686 - val_acc: 0.8810\n",
      "Epoch 134/1000000\n",
      "347/348 [============================>.] - ETA: 0s - loss: 16.1559 - acc: 0.9762\n",
      "Epoch 00134: val_acc did not improve from 0.91337\n",
      "348/348 [==============================] - 56s 160ms/step - loss: 16.1344 - acc: 0.9762 - val_loss: 0.2844 - val_acc: 0.9068\n",
      "Epoch 135/1000000\n",
      " 11/348 [..............................] - ETA: 54s - loss: 16.4954 - acc: 0.9765"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c479d063c092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     callbacks=[checkpoint, tb])\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#tensorboard --logdir=/home/skyolia/JupyterProjects/segmentation/BASE-CYTO/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 171\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2978\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2979\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2980\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs, batch_size, lr, filepath = 1000000, 256, 0.001, \"day_1.weights.best.hdf5\"\n",
    "steps_per_epoch = int(np.ceil(train_y.shape[0]/batch_size))\n",
    "\n",
    "data_gen_args = dict(horizontal_flip=True, vertical_flip=True)#, width_shift_range=0.1, height_shift_range=0.1\n",
    "image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "image_datagen.fit(train_x, augment=True, seed=seed)\n",
    "mask_datagen.fit(train_y, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow(x=train_x, batch_size=batch_size, seed=seed)\n",
    "mask_generator = mask_datagen.flow(x=train_y, batch_size=batch_size, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator().flow(x=test_x, y=test_y, batch_size=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=os.getcwd())\n",
    "\n",
    "opt = tf.keras.optimizers.Adam() # \n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.metrics_names)\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=int(np.ceil(test_y.shape[0]/batch_size)),\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=12,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=[checkpoint, tb])\n",
    "#tensorboard --logdir=/home/skyolia/JupyterProjects/segmentation/BASE-CYTO/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9225624682856541"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = tf.keras.utils.to_categorical(np.argmax(model.predict(x=test_x, batch_size=1), axis=-1), num_classes=4)\n",
    "accuracy_score(np.argmax(test_y, -1).ravel(), np.argmax(pred, -1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
