{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<getitem, shape=(5000, 32, 32, 3), dtype=float64, chunksize=(5000, 32, 32, 3)>,\n",
       " dask.array<getitem, shape=(5000, 1), dtype=float64, chunksize=(5000, 1)>,\n",
       " dask.array<getitem, shape=(1000, 32, 32, 3), dtype=float64, chunksize=(1000, 32, 32, 3)>,\n",
       " dask.array<getitem, shape=(1000, 1), dtype=float64, chunksize=(1000, 1)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/train_x')\n",
    "train_y = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/train_y')\n",
    "test_x = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/test_x')\n",
    "test_y = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/test_y')\n",
    "train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "class SGDR(tf.keras.callbacks.Callback): \n",
    "\n",
    "    def __init__(self, on_start_lenght, min_lr, max_lr, min_mom, max_mom, cycle_length, steps_per_epoch, \n",
    "                 mult_factor=1, lr_decay=1):\n",
    "        self.on_start_lenght = on_start_lenght\n",
    "        self.min_lr, self.max_lr = min_lr, max_lr\n",
    "        self.min_mom, self.max_mom = min_mom, max_mom\n",
    "        self.cycle_length = cycle_length\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        #self.next_restart = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "        self.lr_decay = lr_decay\n",
    "        self.batch_since_restart = 0\n",
    "        self.batch_counter = 0\n",
    "        \n",
    "    def update_lr_mom(self): #update learning rate after each batch\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        mom = self.min_mom + 0.5 * (self.max_mom - self.min_mom) * (1 - np.cos(fraction_to_restart * np.pi))\n",
    "        return lr, mom\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.batch_counter += 1\n",
    "        if self.batch_counter + 1 >= self.on_start_lenght * self.steps_per_epoch:\n",
    "            self.batch_since_restart += 1\n",
    "            lr, mom = self.update_lr_mom()\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "            tf.keras.backend.set_value(self.model.optimizer.momentum, mom)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch + 1 <= self.on_start_lenght:\n",
    "            self.next_restart = self.cycle_length + self.on_start_lenght\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "        \n",
    "class LRTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    '''\n",
    "    Add learning rate evolution to Tensorboard\n",
    "    '''\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__(log_dir=log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs['lr'] = tf.keras.backend.eval(self.model.optimizer.lr)\n",
    "        logs['momentum'] = tf.keras.backend.eval(self.model.optimizer.momentum)\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def build_block(input_layer, filters, norm=True, k=[3,3]):\n",
    "    layer = tf.keras.layers.Conv2D(filters, kernel_size=(k[0], k[1]), padding='same', use_bias=not norm, kernel_initializer='glorot_uniform')(input_layer)\n",
    "    if norm:\n",
    "        layer = tf.keras.layers.BatchNormalization()(layer)\n",
    "    layer = tf.keras.layers.Activation('elu')(layer)\n",
    "    return layer\n",
    "\n",
    "def build_model(num_class):\n",
    "    image_input = tf.keras.Input(shape=(32, 32, 3), name='input_layer')\n",
    "    conv_1 = build_block(image_input, 48)\n",
    "    conv_2 = build_block(conv_1, 48)\n",
    "    pool_1 = tf.keras.layers.MaxPooling2D(padding='same')(conv_2)\n",
    "    drop_1 = tf.keras.layers.Dropout(0.6)(pool_1)\n",
    "    conv_3 = build_block(drop_1, 96)\n",
    "    conv_4 = build_block(conv_3, 96)\n",
    "    pool_2 = tf.keras.layers.MaxPooling2D(padding='same')(conv_4)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.6)(pool_2)\n",
    "    conv_5 = build_block(drop_2, 192)\n",
    "    conv_6 = build_block(conv_5, 192)\n",
    "    pool_3 = tf.keras.layers.MaxPooling2D(padding='same')(conv_6)\n",
    "    drop_3 = tf.keras.layers.Dropout(0.6)(pool_3)\n",
    "    conv_7 = build_block(drop_3, 192, False, [1,1])\n",
    "    drop_4 = tf.keras.layers.Dropout(0.25)(conv_7)\n",
    "    gap = tf.keras.layers.GlobalAvgPool2D()(drop_4)\n",
    "    logits = tf.keras.layers.Dense(units=num_class, activation='softmax', bias_initializer='ones', kernel_initializer='glorot_uniform')(gap)\n",
    "    model = tf.keras.Model(inputs=image_input, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 48)        1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 48)        20736     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 96)        41472     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 96)        82944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 192)         165888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 192)         331776    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 192)         37056     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1930      \n",
      "=================================================================\n",
      "Total params: 685,786\n",
      "Trainable params: 684,442\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"day_1.weights.best.hdf5\")\n",
    "lr=0.001\n",
    "adam = tf.keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "scores = model.evaluate(test_x, test_y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 2.8007 - acc: 0.1677\n",
      "Epoch 00001: val_acc improved from -inf to 0.11900, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 10s 489ms/step - loss: 2.7650 - acc: 0.1681 - val_loss: 4.4979 - val_acc: 0.1190\n",
      "Epoch 2/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.9931 - acc: 0.2479\n",
      "Epoch 00002: val_acc improved from 0.11900 to 0.13100, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 179ms/step - loss: 1.9887 - acc: 0.2483 - val_loss: 3.2821 - val_acc: 0.1310\n",
      "Epoch 3/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.8540 - acc: 0.2895\n",
      "Epoch 00003: val_acc improved from 0.13100 to 0.25600, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 178ms/step - loss: 1.8483 - acc: 0.2938 - val_loss: 2.0316 - val_acc: 0.2560\n",
      "Epoch 4/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.7446 - acc: 0.3398\n",
      "Epoch 00004: val_acc improved from 0.25600 to 0.26000, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 1.7458 - acc: 0.3401 - val_loss: 2.1896 - val_acc: 0.2600\n",
      "Epoch 5/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.6877 - acc: 0.3603\n",
      "Epoch 00005: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 180ms/step - loss: 1.6854 - acc: 0.3617 - val_loss: 2.3882 - val_acc: 0.2380\n",
      "Epoch 6/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.6171 - acc: 0.3914\n",
      "Epoch 00006: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 177ms/step - loss: 1.6148 - acc: 0.3922 - val_loss: 2.4539 - val_acc: 0.2340\n",
      "Epoch 7/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.6044 - acc: 0.3959\n",
      "Epoch 00007: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 183ms/step - loss: 1.6010 - acc: 0.3983 - val_loss: 2.6234 - val_acc: 0.1940\n",
      "Epoch 8/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5817 - acc: 0.4013\n",
      "Epoch 00008: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 192ms/step - loss: 1.5869 - acc: 0.3979 - val_loss: 2.6218 - val_acc: 0.2040\n",
      "Epoch 9/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5801 - acc: 0.4073\n",
      "Epoch 00009: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 180ms/step - loss: 1.5778 - acc: 0.4083 - val_loss: 2.4322 - val_acc: 0.2280\n",
      "Epoch 10/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5779 - acc: 0.4097\n",
      "Epoch 00010: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 180ms/step - loss: 1.5727 - acc: 0.4127 - val_loss: 2.5133 - val_acc: 0.2030\n",
      "Epoch 11/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5586 - acc: 0.4196\n",
      "Epoch 00011: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 180ms/step - loss: 1.5594 - acc: 0.4207 - val_loss: 2.6494 - val_acc: 0.1910\n",
      "Epoch 12/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5393 - acc: 0.4139\n",
      "Epoch 00012: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 177ms/step - loss: 1.5428 - acc: 0.4128 - val_loss: 2.3603 - val_acc: 0.2370\n",
      "Epoch 13/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5295 - acc: 0.4325\n",
      "Epoch 00013: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 185ms/step - loss: 1.5274 - acc: 0.4321 - val_loss: 2.3418 - val_acc: 0.2370\n",
      "Epoch 14/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5285 - acc: 0.4316\n",
      "Epoch 00014: val_acc did not improve from 0.26000\n",
      "20/20 [==============================] - 4s 185ms/step - loss: 1.5264 - acc: 0.4312 - val_loss: 2.2882 - val_acc: 0.2490\n",
      "Epoch 15/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5086 - acc: 0.4352\n",
      "Epoch 00015: val_acc improved from 0.26000 to 0.26600, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 1.5082 - acc: 0.4373 - val_loss: 2.1341 - val_acc: 0.2660\n",
      "Epoch 16/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5414 - acc: 0.4259\n",
      "Epoch 00016: val_acc improved from 0.26600 to 0.29300, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 186ms/step - loss: 1.5373 - acc: 0.4269 - val_loss: 2.0014 - val_acc: 0.2930\n",
      "Epoch 17/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5277 - acc: 0.4224\n",
      "Epoch 00017: val_acc improved from 0.29300 to 0.32700, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 187ms/step - loss: 1.5286 - acc: 0.4247 - val_loss: 1.9576 - val_acc: 0.3270\n",
      "Epoch 18/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5100 - acc: 0.4367\n",
      "Epoch 00018: val_acc did not improve from 0.32700\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 1.5136 - acc: 0.4356 - val_loss: 2.0664 - val_acc: 0.2700\n",
      "Epoch 19/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.5079 - acc: 0.4320\n",
      "Epoch 00019: val_acc improved from 0.32700 to 0.35800, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 192ms/step - loss: 1.5090 - acc: 0.4324 - val_loss: 1.7624 - val_acc: 0.3580\n",
      "Epoch 20/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4908 - acc: 0.4485\n",
      "Epoch 00020: val_acc improved from 0.35800 to 0.36000, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 178ms/step - loss: 1.4902 - acc: 0.4505 - val_loss: 1.6878 - val_acc: 0.3600\n",
      "Epoch 21/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4770 - acc: 0.4515\n",
      "Epoch 00021: val_acc did not improve from 0.36000\n",
      "20/20 [==============================] - 4s 180ms/step - loss: 1.4749 - acc: 0.4531 - val_loss: 1.7946 - val_acc: 0.3500\n",
      "Epoch 22/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4723 - acc: 0.4567\n",
      "Epoch 00022: val_acc improved from 0.36000 to 0.38600, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 184ms/step - loss: 1.4711 - acc: 0.4572 - val_loss: 1.7290 - val_acc: 0.3860\n",
      "Epoch 23/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4575 - acc: 0.4691\n",
      "Epoch 00023: val_acc improved from 0.38600 to 0.38700, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 1.4540 - acc: 0.4702 - val_loss: 1.7008 - val_acc: 0.3870\n",
      "Epoch 24/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4475 - acc: 0.4608\n",
      "Epoch 00024: val_acc improved from 0.38700 to 0.39200, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 192ms/step - loss: 1.4480 - acc: 0.4612 - val_loss: 1.6687 - val_acc: 0.3920\n",
      "Epoch 25/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4343 - acc: 0.4684\n",
      "Epoch 00025: val_acc improved from 0.39200 to 0.42200, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 187ms/step - loss: 1.4356 - acc: 0.4690 - val_loss: 1.5973 - val_acc: 0.4220\n",
      "Epoch 26/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4626 - acc: 0.4565\n",
      "Epoch 00026: val_acc did not improve from 0.42200\n",
      "20/20 [==============================] - 3s 174ms/step - loss: 1.4614 - acc: 0.4586 - val_loss: 1.5978 - val_acc: 0.4020\n",
      "Epoch 27/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4472 - acc: 0.4597\n",
      "Epoch 00027: val_acc did not improve from 0.42200\n",
      "20/20 [==============================] - 3s 174ms/step - loss: 1.4474 - acc: 0.4589 - val_loss: 1.8759 - val_acc: 0.3420\n",
      "Epoch 28/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4419 - acc: 0.4636\n",
      "Epoch 00028: val_acc did not improve from 0.42200\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 1.4409 - acc: 0.4643 - val_loss: 1.9144 - val_acc: 0.3410\n",
      "Epoch 29/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4275 - acc: 0.4662\n",
      "Epoch 00029: val_acc did not improve from 0.42200\n",
      "20/20 [==============================] - 4s 179ms/step - loss: 1.4284 - acc: 0.4649 - val_loss: 1.6238 - val_acc: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.4222 - acc: 0.4726\n",
      "Epoch 00030: val_acc improved from 0.42200 to 0.43700, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 1.4177 - acc: 0.4750 - val_loss: 1.5671 - val_acc: 0.4370\n",
      "Epoch 31/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3997 - acc: 0.4836\n",
      "Epoch 00031: val_acc improved from 0.43700 to 0.47900, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 1.3950 - acc: 0.4858 - val_loss: 1.4321 - val_acc: 0.4790\n",
      "Epoch 32/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3849 - acc: 0.4929\n",
      "Epoch 00032: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 3s 175ms/step - loss: 1.3786 - acc: 0.4959 - val_loss: 1.4568 - val_acc: 0.4630\n",
      "Epoch 33/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3631 - acc: 0.4967\n",
      "Epoch 00033: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 4s 179ms/step - loss: 1.3699 - acc: 0.4929 - val_loss: 1.5734 - val_acc: 0.4450\n",
      "Epoch 34/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3580 - acc: 0.4965\n",
      "Epoch 00034: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 4s 176ms/step - loss: 1.3582 - acc: 0.4954 - val_loss: 1.4936 - val_acc: 0.4640\n",
      "Epoch 35/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3323 - acc: 0.5094\n",
      "Epoch 00035: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 3s 175ms/step - loss: 1.3326 - acc: 0.5090 - val_loss: 1.4205 - val_acc: 0.4690\n",
      "Epoch 36/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3765 - acc: 0.4937\n",
      "Epoch 00036: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 1.3829 - acc: 0.4915 - val_loss: 1.5353 - val_acc: 0.4390\n",
      "Epoch 37/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3722 - acc: 0.5030\n",
      "Epoch 00037: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 4s 187ms/step - loss: 1.3738 - acc: 0.5021 - val_loss: 1.4914 - val_acc: 0.4540\n",
      "Epoch 38/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3540 - acc: 0.4995\n",
      "Epoch 00038: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 4s 189ms/step - loss: 1.3522 - acc: 0.5002 - val_loss: 1.5320 - val_acc: 0.4560\n",
      "Epoch 39/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3568 - acc: 0.5051\n",
      "Epoch 00039: val_acc did not improve from 0.47900\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 1.3532 - acc: 0.5079 - val_loss: 1.5990 - val_acc: 0.4190\n",
      "Epoch 40/10000\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 1.3370 - acc: 0.5157\n",
      "Epoch 00040: val_acc improved from 0.47900 to 0.48500, saving model to day_1.weights.best.hdf5\n",
      "20/20 [==============================] - 4s 184ms/step - loss: 1.3386 - acc: 0.5151 - val_loss: 1.3744 - val_acc: 0.4850\n",
      "Epoch 41/10000\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 1.3017 - acc: 0.5230"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-966:\n",
      "Process ForkPoolWorker-969:\n",
      "Process ForkPoolWorker-967:\n",
      "Process ForkPoolWorker-972:\n",
      "Process ForkPoolWorker-974:\n",
      "Process ForkPoolWorker-980:\n",
      "Process ForkPoolWorker-965:\n",
      "Process ForkPoolWorker-961:\n",
      "Process ForkPoolWorker-977:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-963:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-964:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-962:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/skyolia/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    170\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 171\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1827\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2978\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2979\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-262466b8fd95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;31m#initial_epoch=600,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     callbacks=[checkpoint, sgdr_lr, tblr])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0menqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mval_enqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs, min_lr, max_lr, min_mom, max_mom, batch_size = 10000, 0.002, 0.1, 0.3, 0.9, 256\n",
    "steps_per_epoch = int(np.ceil(train_y.shape[0]/batch_size))\n",
    "filepath=\"day_1.weights.best.hdf5\"\n",
    "\n",
    "train_generator = CIFAR10Sequence(train_x, train_y, batch_size)\n",
    "test_generator = CIFAR10Sequence(test_x, test_y, batch_size)\n",
    "tblr = LRTensorBoard(log_dir=os.getcwd())\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "sgdr_lr = SGDR(80, min_lr, max_lr, min_mom, max_mom, 250, steps_per_epoch)\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=max_lr, momentum=max_mom)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#model.load_weights(filepath)\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=int(np.ceil(test_y.shape[0]/batch_size)),\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=12,\n",
    "                    shuffle=True,\n",
    "                    #initial_epoch=600,\n",
    "                    callbacks=[checkpoint, sgdr_lr, tblr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
