{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<from-npy-stack, shape=(50000, 32, 32, 3), dtype=float64, chunksize=(10000, 32, 32, 3)>,\n",
       " dask.array<from-npy-stack, shape=(50000, 1), dtype=float64, chunksize=(10000, 1)>,\n",
       " dask.array<from-npy-stack, shape=(10000, 32, 32, 3), dtype=float64, chunksize=(1000, 32, 32, 3)>,\n",
       " dask.array<from-npy-stack, shape=(10000, 1), dtype=float64, chunksize=(1000, 1)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/train_x')\n",
    "train_y = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/train_y')\n",
    "test_x = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/test_x')\n",
    "test_y = da.from_npy_stack('/home/skyolia/JupyterProjects/classification/cifar_10/data/test_y')\n",
    "train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "        \n",
    "class LRTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    '''\n",
    "    Add learning rate evolution to Tensorboard\n",
    "    '''\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__(log_dir=log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs['lr'] = tf.keras.backend.eval(self.model.optimizer.lr)\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "def build_block(input_layer, filters, norm=True, k=[3,3]):\n",
    "    layer = tf.keras.layers.Conv2D(filters, kernel_size=(k[0], k[1]), padding='same', use_bias=not norm, kernel_initializer='glorot_uniform')(input_layer)\n",
    "    if norm:\n",
    "        layer = tf.keras.layers.BatchNormalization()(layer)\n",
    "    layer = tf.keras.layers.Activation('elu')(layer)\n",
    "    return layer\n",
    "\n",
    "def build_model(num_class):\n",
    "    image_input = tf.keras.Input(shape=(32, 32, 3), name='input_layer')\n",
    "    conv_1 = build_block(image_input, 48)\n",
    "    conv_2 = build_block(conv_1, 48)\n",
    "    pool_1 = tf.keras.layers.MaxPooling2D(padding='same')(conv_2)\n",
    "    drop_1 = tf.keras.layers.Dropout(0.6)(pool_1)\n",
    "    conv_3 = build_block(drop_1, 96)\n",
    "    conv_4 = build_block(conv_3, 96)\n",
    "    pool_2 = tf.keras.layers.MaxPooling2D(padding='same')(conv_4)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.6)(pool_2)\n",
    "    conv_5 = build_block(drop_2, 192)\n",
    "    conv_6 = build_block(conv_5, 192)\n",
    "    pool_3 = tf.keras.layers.MaxPooling2D(padding='same')(conv_6)\n",
    "    drop_3 = tf.keras.layers.Dropout(0.6)(pool_3)\n",
    "    conv_7 = build_block(drop_3, 192, False, [1,1])\n",
    "    drop_4 = tf.keras.layers.Dropout(0.25)(conv_7)\n",
    "    gap = tf.keras.layers.GlobalAvgPool2D()(drop_4)\n",
    "    logits = tf.keras.layers.Dense(units=num_class, activation='softmax', bias_initializer='ones', kernel_initializer='glorot_uniform')(gap)\n",
    "    model = tf.keras.Model(inputs=image_input, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 48)        1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 48)        20736     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 96)        41472     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 96)        82944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 192)         165888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 192)         331776    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 192)         37056     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1930      \n",
      "=================================================================\n",
      "Total params: 685,786\n",
      "Trainable params: 684,442\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 19s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37976756334006784, 0.8966]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"day_1.weights.best.hdf5\")\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "scores = model.evaluate(test_x, test_y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 10.7208 - acc: 0.1260\n",
      "Epoch 00001: val_acc improved from -inf to 0.11550, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 46s 237ms/step - loss: 10.7047 - acc: 0.1259 - val_loss: 6.8109 - val_acc: 0.1155\n",
      "Epoch 2/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.2678 - acc: 0.2173\n",
      "Epoch 00002: val_acc improved from 0.11550 to 0.20350, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 32s 161ms/step - loss: 3.2611 - acc: 0.2175 - val_loss: 2.5739 - val_acc: 0.2035\n",
      "Epoch 3/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.8350 - acc: 0.3027\n",
      "Epoch 00003: val_acc improved from 0.20350 to 0.27100, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 32s 165ms/step - loss: 1.8346 - acc: 0.3028 - val_loss: 2.0170 - val_acc: 0.2710\n",
      "Epoch 4/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.7220 - acc: 0.3509\n",
      "Epoch 00004: val_acc improved from 0.27100 to 0.33800, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 168ms/step - loss: 1.7215 - acc: 0.3511 - val_loss: 1.8030 - val_acc: 0.3380\n",
      "Epoch 5/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.6379 - acc: 0.3850\n",
      "Epoch 00005: val_acc improved from 0.33800 to 0.34730, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 1.6376 - acc: 0.3852 - val_loss: 1.9037 - val_acc: 0.3473\n",
      "Epoch 6/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.5564 - acc: 0.4206\n",
      "Epoch 00006: val_acc improved from 0.34730 to 0.41690, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 168ms/step - loss: 1.5561 - acc: 0.4206 - val_loss: 1.5633 - val_acc: 0.4169\n",
      "Epoch 7/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.4816 - acc: 0.4495\n",
      "Epoch 00007: val_acc improved from 0.41690 to 0.46860, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 169ms/step - loss: 1.4816 - acc: 0.4495 - val_loss: 1.4399 - val_acc: 0.4686\n",
      "Epoch 8/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.4032 - acc: 0.4792\n",
      "Epoch 00008: val_acc improved from 0.46860 to 0.49760, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 169ms/step - loss: 1.4027 - acc: 0.4795 - val_loss: 1.3639 - val_acc: 0.4976\n",
      "Epoch 9/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.3210 - acc: 0.5139\n",
      "Epoch 00009: val_acc improved from 0.49760 to 0.57550, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 171ms/step - loss: 1.3207 - acc: 0.5138 - val_loss: 1.1797 - val_acc: 0.5755\n",
      "Epoch 10/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.2517 - acc: 0.5434\n",
      "Epoch 00010: val_acc improved from 0.57550 to 0.58370, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 171ms/step - loss: 1.2514 - acc: 0.5436 - val_loss: 1.1288 - val_acc: 0.5837\n",
      "Epoch 11/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.1854 - acc: 0.5678\n",
      "Epoch 00011: val_acc improved from 0.58370 to 0.59360, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 171ms/step - loss: 1.1852 - acc: 0.5679 - val_loss: 1.1270 - val_acc: 0.5936\n",
      "Epoch 12/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.1400 - acc: 0.5895\n",
      "Epoch 00012: val_acc improved from 0.59360 to 0.61810, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 33s 171ms/step - loss: 1.1398 - acc: 0.5894 - val_loss: 1.0464 - val_acc: 0.6181\n",
      "Epoch 13/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.6049\n",
      "Epoch 00013: val_acc did not improve from 0.61810\n",
      "196/196 [==============================] - 33s 171ms/step - loss: 1.0928 - acc: 0.6052 - val_loss: 1.1453 - val_acc: 0.5803\n",
      "Epoch 14/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.0614 - acc: 0.6191\n",
      "Epoch 00014: val_acc did not improve from 0.61810\n",
      "196/196 [==============================] - 34s 171ms/step - loss: 1.0618 - acc: 0.6189 - val_loss: 1.3822 - val_acc: 0.5550\n",
      "Epoch 15/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.0293 - acc: 0.6288\n",
      "Epoch 00015: val_acc improved from 0.61810 to 0.62570, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 1.0289 - acc: 0.6291 - val_loss: 1.0507 - val_acc: 0.6257\n",
      "Epoch 16/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.9953 - acc: 0.6433\n",
      "Epoch 00016: val_acc improved from 0.62570 to 0.63040, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.9955 - acc: 0.6433 - val_loss: 1.0485 - val_acc: 0.6304\n",
      "Epoch 17/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.9622 - acc: 0.6550\n",
      "Epoch 00017: val_acc did not improve from 0.63040\n",
      "196/196 [==============================] - 34s 172ms/step - loss: 0.9624 - acc: 0.6549 - val_loss: 1.1153 - val_acc: 0.6283\n",
      "Epoch 18/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.9393 - acc: 0.6627\n",
      "Epoch 00018: val_acc improved from 0.63040 to 0.65890, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 172ms/step - loss: 0.9395 - acc: 0.6626 - val_loss: 0.9239 - val_acc: 0.6589\n",
      "Epoch 19/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.8990 - acc: 0.6796\n",
      "Epoch 00019: val_acc improved from 0.65890 to 0.71370, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.8989 - acc: 0.6796 - val_loss: 0.8132 - val_acc: 0.7137\n",
      "Epoch 20/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.8749 - acc: 0.6880\n",
      "Epoch 00020: val_acc did not improve from 0.71370\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.8751 - acc: 0.6878 - val_loss: 0.9690 - val_acc: 0.6683\n",
      "Epoch 21/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.8536 - acc: 0.6935\n",
      "Epoch 00021: val_acc did not improve from 0.71370\n",
      "196/196 [==============================] - 34s 172ms/step - loss: 0.8533 - acc: 0.6936 - val_loss: 0.8411 - val_acc: 0.7107\n",
      "Epoch 22/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7060\n",
      "Epoch 00022: val_acc improved from 0.71370 to 0.73730, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 172ms/step - loss: 0.8305 - acc: 0.7056 - val_loss: 0.7546 - val_acc: 0.7373\n",
      "Epoch 23/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.8095 - acc: 0.7127\n",
      "Epoch 00023: val_acc improved from 0.73730 to 0.74050, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.8096 - acc: 0.7126 - val_loss: 0.7506 - val_acc: 0.7405\n",
      "Epoch 24/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.7902 - acc: 0.7210\n",
      "Epoch 00024: val_acc improved from 0.74050 to 0.74730, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.7896 - acc: 0.7212 - val_loss: 0.7293 - val_acc: 0.7473\n",
      "Epoch 25/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.7686 - acc: 0.7294\n",
      "Epoch 00025: val_acc improved from 0.74730 to 0.74830, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.7687 - acc: 0.7294 - val_loss: 0.7365 - val_acc: 0.7483\n",
      "Epoch 26/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.7541 - acc: 0.7320\n",
      "Epoch 00026: val_acc improved from 0.74830 to 0.75050, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.7541 - acc: 0.7319 - val_loss: 0.7098 - val_acc: 0.7505\n",
      "Epoch 27/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.7368 - acc: 0.7399\n",
      "Epoch 00027: val_acc improved from 0.75050 to 0.75760, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.7369 - acc: 0.7399 - val_loss: 0.6826 - val_acc: 0.7576\n",
      "Epoch 28/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.7183 - acc: 0.7463\n",
      "Epoch 00028: val_acc improved from 0.75760 to 0.76060, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.7182 - acc: 0.7463 - val_loss: 0.6748 - val_acc: 0.7606\n",
      "Epoch 29/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.7041 - acc: 0.7526\n",
      "Epoch 00029: val_acc did not improve from 0.76060\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.7042 - acc: 0.7525 - val_loss: 0.8896 - val_acc: 0.7064\n",
      "Epoch 30/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6936 - acc: 0.7562\n",
      "Epoch 00030: val_acc did not improve from 0.76060\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.6938 - acc: 0.7560 - val_loss: 0.7751 - val_acc: 0.7391\n",
      "Epoch 31/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.7635\n",
      "Epoch 00031: val_acc improved from 0.76060 to 0.77110, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6790 - acc: 0.7635 - val_loss: 0.6566 - val_acc: 0.7711\n",
      "Epoch 32/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6723 - acc: 0.7641\n",
      "Epoch 00032: val_acc did not improve from 0.77110\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.6721 - acc: 0.7641 - val_loss: 0.7687 - val_acc: 0.7433\n",
      "Epoch 33/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6566 - acc: 0.7701\n",
      "Epoch 00033: val_acc improved from 0.77110 to 0.79640, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6570 - acc: 0.7702 - val_loss: 0.5979 - val_acc: 0.7964\n",
      "Epoch 34/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.7737\n",
      "Epoch 00034: val_acc did not improve from 0.79640\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6500 - acc: 0.7737 - val_loss: 0.7052 - val_acc: 0.7670\n",
      "Epoch 35/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6372 - acc: 0.7775\n",
      "Epoch 00035: val_acc did not improve from 0.79640\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6368 - acc: 0.7777 - val_loss: 0.6328 - val_acc: 0.7882\n",
      "Epoch 36/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.7821\n",
      "Epoch 00036: val_acc improved from 0.79640 to 0.81030, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6223 - acc: 0.7822 - val_loss: 0.5530 - val_acc: 0.8103\n",
      "Epoch 37/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6194 - acc: 0.7825\n",
      "Epoch 00037: val_acc did not improve from 0.81030\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6194 - acc: 0.7825 - val_loss: 0.6508 - val_acc: 0.7764\n",
      "Epoch 38/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.7879\n",
      "Epoch 00038: val_acc did not improve from 0.81030\n",
      "196/196 [==============================] - 34s 175ms/step - loss: 0.6035 - acc: 0.7879 - val_loss: 0.5621 - val_acc: 0.8084\n",
      "Epoch 39/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.7871\n",
      "Epoch 00039: val_acc did not improve from 0.81030\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.6055 - acc: 0.7868 - val_loss: 0.6609 - val_acc: 0.7821\n",
      "Epoch 40/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.7940\n",
      "Epoch 00040: val_acc improved from 0.81030 to 0.81480, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 175ms/step - loss: 0.5903 - acc: 0.7941 - val_loss: 0.5478 - val_acc: 0.8148\n",
      "Epoch 41/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5839 - acc: 0.7963\n",
      "Epoch 00041: val_acc did not improve from 0.81480\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5837 - acc: 0.7964 - val_loss: 0.6162 - val_acc: 0.7929\n",
      "Epoch 42/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5764 - acc: 0.7975\n",
      "Epoch 00042: val_acc did not improve from 0.81480\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5769 - acc: 0.7974 - val_loss: 0.7493 - val_acc: 0.7598\n",
      "Epoch 43/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8033\n",
      "Epoch 00043: val_acc did not improve from 0.81480\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5628 - acc: 0.8033 - val_loss: 0.5603 - val_acc: 0.8110\n",
      "Epoch 44/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.8003\n",
      "Epoch 00044: val_acc improved from 0.81480 to 0.83030, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5691 - acc: 0.8002 - val_loss: 0.4962 - val_acc: 0.8303\n",
      "Epoch 45/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8052\n",
      "Epoch 00045: val_acc did not improve from 0.83030\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5562 - acc: 0.8051 - val_loss: 0.5662 - val_acc: 0.8124\n",
      "Epoch 46/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8092\n",
      "Epoch 00046: val_acc did not improve from 0.83030\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.5458 - acc: 0.8092 - val_loss: 0.5238 - val_acc: 0.8235\n",
      "Epoch 47/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.8089\n",
      "Epoch 00047: val_acc did not improve from 0.83030\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.5422 - acc: 0.8090 - val_loss: 0.5262 - val_acc: 0.8229\n",
      "Epoch 48/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8122\n",
      "Epoch 00048: val_acc did not improve from 0.83030\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5371 - acc: 0.8123 - val_loss: 0.5579 - val_acc: 0.8164\n",
      "Epoch 49/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.8160\n",
      "Epoch 00049: val_acc did not improve from 0.83030\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5265 - acc: 0.8161 - val_loss: 0.6186 - val_acc: 0.7967\n",
      "Epoch 50/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.8150\n",
      "Epoch 00050: val_acc improved from 0.83030 to 0.83650, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 34s 175ms/step - loss: 0.5261 - acc: 0.8152 - val_loss: 0.4947 - val_acc: 0.8365\n",
      "Epoch 51/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.8185\n",
      "Epoch 00051: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5165 - acc: 0.8186 - val_loss: 0.5130 - val_acc: 0.8310\n",
      "Epoch 52/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8200\n",
      "Epoch 00052: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.5130 - acc: 0.8203 - val_loss: 0.5035 - val_acc: 0.8304\n",
      "Epoch 53/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.8244\n",
      "Epoch 00053: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 175ms/step - loss: 0.5062 - acc: 0.8243 - val_loss: 0.5538 - val_acc: 0.8151\n",
      "Epoch 54/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8250\n",
      "Epoch 00054: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 175ms/step - loss: 0.5017 - acc: 0.8251 - val_loss: 0.4953 - val_acc: 0.8326\n",
      "Epoch 55/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8258\n",
      "Epoch 00055: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 174ms/step - loss: 0.4958 - acc: 0.8258 - val_loss: 0.5635 - val_acc: 0.8190\n",
      "Epoch 56/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8264\n",
      "Epoch 00056: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.4955 - acc: 0.8262 - val_loss: 0.5569 - val_acc: 0.8160\n",
      "Epoch 57/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8310\n",
      "Epoch 00057: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 34s 173ms/step - loss: 0.4882 - acc: 0.8310 - val_loss: 0.4927 - val_acc: 0.8360\n",
      "Epoch 58/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.8304\n",
      "Epoch 00058: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 35s 178ms/step - loss: 0.4836 - acc: 0.8307 - val_loss: 0.5424 - val_acc: 0.8264\n",
      "Epoch 59/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.8309\n",
      "Epoch 00059: val_acc did not improve from 0.83650\n",
      "196/196 [==============================] - 35s 179ms/step - loss: 0.4794 - acc: 0.8308 - val_loss: 0.4975 - val_acc: 0.8344\n",
      "Epoch 60/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8350\n",
      "Epoch 00060: val_acc improved from 0.83650 to 0.84240, saving model to day_1.weights.best.hdf5\n",
      "196/196 [==============================] - 35s 178ms/step - loss: 0.4717 - acc: 0.8351 - val_loss: 0.4765 - val_acc: 0.8424\n",
      "Epoch 61/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4665 - acc: 0.8370\n",
      "Epoch 00061: val_acc did not improve from 0.84240\n",
      "196/196 [==============================] - 34s 175ms/step - loss: 0.4666 - acc: 0.8371 - val_loss: 0.5189 - val_acc: 0.8297\n",
      "Epoch 62/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8376\n",
      "Epoch 00062: val_acc did not improve from 0.84240\n",
      "196/196 [==============================] - 35s 177ms/step - loss: 0.4627 - acc: 0.8376 - val_loss: 0.4972 - val_acc: 0.8385\n",
      "Epoch 63/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4665 - acc: 0.8364\n",
      "Epoch 00063: val_acc did not improve from 0.84240\n",
      "196/196 [==============================] - 36s 184ms/step - loss: 0.4665 - acc: 0.8364 - val_loss: 0.5077 - val_acc: 0.8316\n",
      "Epoch 64/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8387\n",
      "Epoch 00064: val_acc did not improve from 0.84240\n",
      "196/196 [==============================] - 36s 184ms/step - loss: 0.4565 - acc: 0.8387 - val_loss: 0.5144 - val_acc: 0.8314\n",
      "Epoch 65/10000\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8398\n",
      "Epoch 00065: val_acc did not improve from 0.84240\n",
      "196/196 [==============================] - 36s 181ms/step - loss: 0.4499 - acc: 0.8397 - val_loss: 0.4896 - val_acc: 0.8389\n",
      "Epoch 66/10000\n",
      " 51/196 [======>.......................] - ETA: 24s - loss: 0.4485 - acc: 0.8414"
     ]
    }
   ],
   "source": [
    "epochs, lr, batch_size = 10000, 1.5, 256\n",
    "steps_per_epoch = int(np.ceil(train_y.shape[0]/batch_size))\n",
    "filepath=\"day_1.weights.best.hdf5\"\n",
    "\n",
    "train_generator = CIFAR10Sequence(train_x, train_y, batch_size)\n",
    "test_generator = CIFAR10Sequence(test_x, test_y, batch_size)\n",
    "tblr = LRTensorBoard(log_dir=os.getcwd())\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=0.008, mode='min', cooldown=1, verbose=1)\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=lr) # \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#model.load_weights(filepath)\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=int(np.ceil(test_y.shape[0]/batch_size)),\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=12,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=65,\n",
    "                    callbacks=[checkpoint, reduce_lr, tblr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
